<html>
<head>
  <style>
  body {
    background-image: url("backgroundimg.png") ;
    background-color: #C2DDCF;
    margin: 125px;
    font-family: Verdana, sans-serif;
  }
  #footnotes , #writing , #header {
    border: 5px solid #FFE7E0;
    border-radius: 5px;
    background-color: #FFD1C3;
    color: #89B39E;
    text-align: justify;
    padding: 15px;
    margin: 30px;
  }
  p {
    text-indent: 45px;
    line-height: 150%;
  }
  a {
    color: #89B39E;
  }
  </style>
</head>
<body>
<div id="header">
  <h2> Ethical Reflection 1: Machine Bias <h2>
</div>
<div id="writing">
  <p > July 11th, 2018 </p>
  <p> In this day and age, technology is advancing rapidly and being applied to aspects of one’s daily life in a variety of ways. From swiping a key card to open a door or using a fingerprint to unlock a phone, these advancements are generally advantageous to society. However, the science becomes questionable when algorithms are a factor in the prosecution of a criminal. A machine should not be able to hold such power in the process– even though the computer is programmed to be as accurate as possible, it will not always have precise predictions, especially when there is evidence of a racial bias within the results.
  </p>
  <p> In May of 2016, Propublica, a newsroom focusing on investigative journalism, published an article titled “Machine Bias” that examined a computer program used to predict the risk of a person committing future crimes <a href="https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing" > (1) </a> . This algorithm, called the Correctional Offender Management Profiling for Alternative Sanctions– or COMPAS for short– uses a person’s responses to a 137-part questionnaire in order to predict their recidivism <a href="https://www.washingtonpost.com/news/monkey-cage/wp/2016/10/17/can-an-algorithm-be-racist-our-analysis-is-more-cautious-than-propublicas/?noredirect=on&utm_term=.92eb05b524b0" > (2) </a>. In theory, this software could be useful for judges when making decisions having to do with bond amounts and jail sentences, yet in practice, the risk assessments may result in an unjust disposition. As seen in the Propublica article, there have been instances of people being arrested and receiving high risk scores that do not go on to commit any more offenses. As it turns out, the software was more likely to identify black people as likely to reoffend, while white people were mislabeled more often than their black counterparts.  In an ideal world, COMPAS would accurate enough for it to be a reliable factor for a judge to determine their ruling. Due to the way that it is programmed, the algorithm was able to identify many people accurately, but still had a high number of false positives <a href="https://www.technologyreview.com/s/607955/inspecting-algorithms-for-bias/" > (3) </a>. Decisions made within the criminal justice system can change a person’s life drastically, and the purpose of going on trial is to receive a just punishment for a crime. A high risk assessment may be the component that induces the judge to give a defendant a longer jail sentence. But if it was a mislabeled high-risk, the ruling is nothing but unfair to the individual.
  </p>
  <p> Until there is technology advanced enough to accurately identify people almost every single time, computer programs such as COMPAS should not be given to judges. The software could still assess people, but the data should be used solely for the purpose of improving the program. Modifications should be made such that there is no racial bias and the likelihood of mislabeling amongst races is equal. Until then, humans should make decisions based on the evidence they have, and not the assumptions that a computer has made about the defendant.
  </p>
</div>
<div id="footnotes">
<ol>
  <li> <a href="https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing" > Machine Bias, <em> Propublica </em> </a> </li>
  <li> <a href="https://www.washingtonpost.com/news/monkey-cage/wp/2016/10/17/can-an-algorithm-be-racist-our-analysis-is-more-cautious-than-propublicas/?noredirect=on&utm_term=.92eb05b524b0" > A computer program used for bail and sentencing decisions was labeled biased against blacks, <em> The Washington Post </em> </a> </li>
  <li> <a href="https://www.technologyreview.com/s/607955/inspecting-algorithms-for-bias/" > <em> Inspecting Algorithms for Bias, MIT Technology Review </em> </a></li>
</ol>
</div>
</body>

</html>
